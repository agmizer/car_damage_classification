{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2cff07",
   "metadata": {},
   "source": [
    "## Car Damage Severity Classification\n",
    "\n",
    "This notebook implements a deep learning pipeline based on the DINOv3 ConvNeXt-Small backbone to classify car damage severity into minor, moderate, and severe. The dataset is made by Prajwal Bhamere and comes from https://www.kaggle.com/datasets/prajwalbhamere/car-damage-severity-dataset/data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568f491",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "base_model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    device_map=\"auto\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2590d",
   "metadata": {},
   "source": [
    "## Wrapper Module\n",
    "Let's make a wrapper module to wrap the original model so we can expose our own forward method to return just the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc164d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDamageClassifier(nn.Module):\n",
    "    def __init__(self, backbone, feature_dim: int = 768, hidden_dim = 256, dropout = 0.3):\n",
    "        \"\"\"\n",
    "        Creates a torch.nn.Module for our car damage classifier.\n",
    "        \n",
    "        Parameters:\n",
    "            feature_dim: int = 768 - Dimension of the input feature (DINOv3-ConvNext is 768)\n",
    "            hidden_dim: int = 256 - Hidden layer dimensions\n",
    "            dropout: float = 0.3 - Dropout Rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.backbone(images).pooler_output\n",
    "        logits = self.head(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294775c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CarDamageClassifier(base_model, feature_dim=768)\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef42c2",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Let's load our data in and apply some augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3236df7",
   "metadata": {},
   "source": [
    "### Augmentations\n",
    "\n",
    "First, let's decide which augmentations we actually need.\n",
    "\n",
    "For both training and validation, we'll resize to 256x256.\n",
    "\n",
    "For training, we’ll choose a more aggressive approach.\n",
    "\n",
    "* **Random Horizontal Flips**\n",
    "* **Color Jitter** (brightness, contrast, saturation, hue) so it doesn’t overfit to a specific lighting.\n",
    "* **Mild Gaussian blur or small rotations** to simulate motion blur / slight camera tilt.\n",
    "* **Normalization using Imagenet mean and standard deviation**, matching what was used to pretrain the DINOv3 ConvNeXt backbone.\n",
    "\n",
    "For validation, there are no augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c285ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a8dfd",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Now, we can make use of torch's `ImageFolder` to load our data in and automatically apply augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\"../dataset/train\", transform=train_transform)\n",
    "val_dataset   = ImageFolder(\"../dataset/val\",   transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8d006",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "\n",
    "Now we can shift to training our model. We need to configure some of the backing training configurations, such as loss, learning rate, epochs, batch size, etc. \n",
    "\n",
    "Once we define our batch size, we can also create our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "optimizer = AdamW(params=model.head.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec2eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c73af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da17ab",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81569f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()  # ensure training mode each epoch\n",
    "\n",
    "    # ----- TRAINING -----\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass → logits\n",
    "        logits = model(images)\n",
    "\n",
    "        # compute loss\n",
    "        computed_loss = loss(logits, labels)\n",
    "\n",
    "        # backward + step\n",
    "        computed_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        train_loss += computed_loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # ----- VALIDATION -----\n",
    "    model.eval()  # Enter evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            computed_loss = loss(logits, labels)\n",
    "\n",
    "            val_loss += computed_loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "        f\"| Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
