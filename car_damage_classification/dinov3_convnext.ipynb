{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2cff07",
   "metadata": {},
   "source": [
    "## Car Damage Severity Classification\n",
    "\n",
    "This notebook implements a deep learning pipeline based on the DINOv3 ConvNeXt-Small backbone to classify car damage severity into minor, moderate, and severe. The dataset is made by Prajwal Bhamere and comes from https://www.kaggle.com/datasets/prajwalbhamere/car-damage-severity-dataset/data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568f491",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "base_model = AutoModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2590d",
   "metadata": {},
   "source": [
    "## Wrapper Module\n",
    "Let's make a wrapper module to wrap the original model so we can expose our own forward method to return just the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc164d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDamageClassifier(nn.Module):\n",
    "    def __init__(self, backbone, feature_dim: int = 768, hidden_dim = 256, dropout = 0.3):\n",
    "        \"\"\"\n",
    "        Creates a torch.nn.Module for our car damage classifier.\n",
    "        \n",
    "        Parameters:\n",
    "            feature_dim: int = 768 - Dimension of the input feature (DINOv3-ConvNext is 768)\n",
    "            hidden_dim: int = 256 - Hidden layer dimensions\n",
    "            dropout: float = 0.3 - Dropout Rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.backbone(images).pooler_output\n",
    "        logits = self.head(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294775c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CarDamageClassifier(\n",
       "  (backbone): DINOv3ConvNextModel(\n",
       "    (stages): ModuleList(\n",
       "      (0): DINOv3ConvNextStage(\n",
       "        (downsample_layers): ModuleList(\n",
       "          (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): DINOv3ConvNextLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x DINOv3ConvNextLayer(\n",
       "            (depthwise_conv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (layer_norm): DINOv3ConvNextLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (pointwise_conv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (pointwise_conv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DINOv3ConvNextStage(\n",
       "        (downsample_layers): ModuleList(\n",
       "          (0): DINOv3ConvNextLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x DINOv3ConvNextLayer(\n",
       "            (depthwise_conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (layer_norm): DINOv3ConvNextLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (pointwise_conv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (pointwise_conv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DINOv3ConvNextStage(\n",
       "        (downsample_layers): ModuleList(\n",
       "          (0): DINOv3ConvNextLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x DINOv3ConvNextLayer(\n",
       "            (depthwise_conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (layer_norm): DINOv3ConvNextLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (pointwise_conv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (pointwise_conv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DINOv3ConvNextStage(\n",
       "        (downsample_layers): ModuleList(\n",
       "          (0): DINOv3ConvNextLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x DINOv3ConvNextLayer(\n",
       "            (depthwise_conv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (layer_norm): DINOv3ConvNextLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (pointwise_conv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (pointwise_conv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CarDamageClassifier(base_model, feature_dim=768, dropout=0.5)\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef42c2",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Let's load our data in and apply some augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3236df7",
   "metadata": {},
   "source": [
    "### Augmentations\n",
    "\n",
    "First, let's decide which augmentations we actually need.\n",
    "\n",
    "For both training and validation, we'll resize to 256x256.\n",
    "\n",
    "For training, we’ll choose a more aggressive approach.\n",
    "\n",
    "* **Random Horizontal Flips**\n",
    "* **Color Jitter** (brightness, contrast, saturation, hue) so it doesn’t overfit to a specific lighting.\n",
    "* **Mild Gaussian blur or small rotations** to simulate motion blur / slight camera tilt.\n",
    "* **Normalization using Imagenet mean and standard deviation**, matching what was used to pretrain the DINOv3 ConvNeXt backbone.\n",
    "\n",
    "For validation, there are no augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c285ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a8dfd",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Now, we can make use of torch's `ImageFolder` to load our data in and automatically apply augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454c2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\"../dataset/train\", transform=train_transform)\n",
    "val_dataset   = ImageFolder(\"../dataset/val\",   transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8d006",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "\n",
    "Now we can shift to training our model. We need to configure some of the backing training configurations, such as loss, learning rate, epochs, batch size, etc. \n",
    "\n",
    "Once we define our batch size, we can also create our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "optimizer = AdamW(params=model.head.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c73af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da17ab",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81569f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 44/44 [03:29<00:00,  4.76s/it]\n",
      "Validation Epoch 1/100: 100%|██████████| 8/8 [00:55<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best val loss: model saved!\n",
      "Epoch [1/100] Train Loss: 0.9851 | Train Acc: 0.5358 | Val Loss: 0.6781 | Val Acc: 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 44/44 [03:34<00:00,  4.88s/it]\n",
      "Validation Epoch 2/100: 100%|██████████| 8/8 [00:57<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best val loss: model saved!\n",
      "Epoch [2/100] Train Loss: 0.7109 | Train Acc: 0.6833 | Val Loss: 0.6441 | Val Acc: 0.7339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 44/44 [03:26<00:00,  4.69s/it]\n",
      "Validation Epoch 3/100: 100%|██████████| 8/8 [00:55<00:00,  6.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Train Loss: 0.6303 | Train Acc: 0.7245 | Val Loss: 0.6474 | Val Acc: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 44/44 [03:28<00:00,  4.73s/it]\n",
      "Validation Epoch 4/100: 100%|██████████| 8/8 [00:55<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Train Loss: 0.5602 | Train Acc: 0.7636 | Val Loss: 0.6625 | Val Acc: 0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 44/44 [03:27<00:00,  4.73s/it]\n",
      "Validation Epoch 5/100: 100%|██████████| 8/8 [00:56<00:00,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Train Loss: 0.5240 | Train Acc: 0.7643 | Val Loss: 0.6522 | Val Acc: 0.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 44/44 [03:25<00:00,  4.67s/it]\n",
      "Validation Epoch 6/100: 100%|██████████| 8/8 [00:55<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Train Loss: 0.4656 | Train Acc: 0.8040 | Val Loss: 0.6736 | Val Acc: 0.7298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 44/44 [03:25<00:00,  4.67s/it]\n",
      "Validation Epoch 7/100: 100%|██████████| 8/8 [00:57<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Train Loss: 0.4293 | Train Acc: 0.8301 | Val Loss: 0.6773 | Val Acc: 0.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 44/44 [03:26<00:00,  4.69s/it]\n",
      "Validation Epoch 8/100: 100%|██████████| 8/8 [00:55<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Train Loss: 0.3993 | Train Acc: 0.8409 | Val Loss: 0.6959 | Val Acc: 0.7339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 44/44 [03:25<00:00,  4.68s/it]\n",
      "Validation Epoch 9/100: 100%|██████████| 8/8 [00:55<00:00,  6.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Train Loss: 0.3621 | Train Acc: 0.8648 | Val Loss: 0.7227 | Val Acc: 0.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 44/44 [03:26<00:00,  4.69s/it]\n",
      "Validation Epoch 10/100: 100%|██████████| 8/8 [00:55<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Train Loss: 0.3468 | Train Acc: 0.8655 | Val Loss: 0.7060 | Val Acc: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 44/44 [03:24<00:00,  4.65s/it]\n",
      "Validation Epoch 11/100: 100%|██████████| 8/8 [00:55<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Train Loss: 0.3397 | Train Acc: 0.8764 | Val Loss: 0.7170 | Val Acc: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 44/44 [03:24<00:00,  4.64s/it]\n",
      "Validation Epoch 12/100: 100%|██████████| 8/8 [00:56<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] Train Loss: 0.3226 | Train Acc: 0.8713 | Val Loss: 0.7480 | Val Acc: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 44/44 [03:24<00:00,  4.64s/it]\n",
      "Validation Epoch 13/100: 100%|██████████| 8/8 [00:55<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Train Loss: 0.2867 | Train Acc: 0.8800 | Val Loss: 0.7935 | Val Acc: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 44/44 [03:23<00:00,  4.63s/it]\n",
      "Validation Epoch 14/100: 100%|██████████| 8/8 [00:55<00:00,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] Train Loss: 0.2696 | Train Acc: 0.8959 | Val Loss: 0.7997 | Val Acc: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 44/44 [03:31<00:00,  4.80s/it]\n",
      "Validation Epoch 15/100: 100%|██████████| 8/8 [00:57<00:00,  7.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] Train Loss: 0.2538 | Train Acc: 0.8973 | Val Loss: 0.8078 | Val Acc: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 44/44 [03:37<00:00,  4.93s/it]\n",
      "Validation Epoch 16/100: 100%|██████████| 8/8 [00:57<00:00,  7.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] Train Loss: 0.2252 | Train Acc: 0.9205 | Val Loss: 0.8509 | Val Acc: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 44/44 [03:34<00:00,  4.86s/it]\n",
      "Validation Epoch 17/100: 100%|██████████| 8/8 [00:58<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] Train Loss: 0.2332 | Train Acc: 0.9132 | Val Loss: 0.8215 | Val Acc: 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 44/44 [03:32<00:00,  4.83s/it]\n",
      "Validation Epoch 18/100: 100%|██████████| 8/8 [00:55<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] Train Loss: 0.2245 | Train Acc: 0.9183 | Val Loss: 0.7935 | Val Acc: 0.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 44/44 [03:29<00:00,  4.77s/it]\n",
      "Validation Epoch 19/100: 100%|██████████| 8/8 [00:57<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] Train Loss: 0.2065 | Train Acc: 0.9262 | Val Loss: 0.8683 | Val Acc: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 44/44 [04:26<00:00,  6.06s/it]\n",
      "Validation Epoch 20/100: 100%|██████████| 8/8 [01:09<00:00,  8.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] Train Loss: 0.1831 | Train Acc: 0.9277 | Val Loss: 0.8663 | Val Acc: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 44/44 [05:04<00:00,  6.91s/it]\n",
      "Validation Epoch 21/100: 100%|██████████| 8/8 [01:08<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] Train Loss: 0.1535 | Train Acc: 0.9501 | Val Loss: 0.8535 | Val Acc: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 44/44 [05:50<00:00,  7.97s/it]\n",
      "Validation Epoch 22/100: 100%|██████████| 8/8 [00:55<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] Train Loss: 0.1714 | Train Acc: 0.9328 | Val Loss: 0.9309 | Val Acc: 0.6976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 44/44 [03:23<00:00,  4.63s/it]\n",
      "Validation Epoch 23/100: 100%|██████████| 8/8 [00:55<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] Train Loss: 0.1596 | Train Acc: 0.9537 | Val Loss: 0.9501 | Val Acc: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 44/44 [03:24<00:00,  4.64s/it]\n",
      "Validation Epoch 24/100: 100%|██████████| 8/8 [00:54<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] Train Loss: 0.1793 | Train Acc: 0.9349 | Val Loss: 0.8882 | Val Acc: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 44/44 [03:25<00:00,  4.67s/it]\n",
      "Validation Epoch 25/100: 100%|██████████| 8/8 [00:55<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] Train Loss: 0.1570 | Train Acc: 0.9429 | Val Loss: 0.9524 | Val Acc: 0.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 44/44 [03:23<00:00,  4.63s/it]\n",
      "Validation Epoch 26/100: 100%|██████████| 8/8 [00:55<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] Train Loss: 0.1369 | Train Acc: 0.9602 | Val Loss: 0.9612 | Val Acc: 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 44/44 [03:29<00:00,  4.76s/it]\n",
      "Validation Epoch 27/100: 100%|██████████| 8/8 [00:57<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] Train Loss: 0.1459 | Train Acc: 0.9537 | Val Loss: 1.0303 | Val Acc: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100:   0%|          | 0/44 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "Epoch 28/100:   0%|          | 0/44 [00:00<?, ?it/s]  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/__init__.py\", line 18, in <module>\n",
      "    import inspect\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/inspect.py\", line 146, in <module>\n",
      "    import dis\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/dis.py\", line 245, in <module>\n",
      "\n",
      "    Positions = collections.namedtuple(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/collections/__init__.py\", line 508, in namedtuple\n",
      "    result = type(typename, (tuple,), class_namespace)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/__init__.py\", line 427, in <module>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/__init__.py\", line 427, in <module>\n",
      "    from torch._C import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
      "KeyboardInterrupt\n",
      "    from torch._C import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model.train()  \u001b[38;5;66;03m# ensure training mode each epoch\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ----- TRAINING -----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/popen_spawn_posix.py:58\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     55\u001b[39m cmd = spawn.get_command_line(tracker_fd=tracker_fd,\n\u001b[32m     56\u001b[39m                              pipe_handle=child_r)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m._fds.extend([child_r, child_w])\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawnv_passfds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_executable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/py312/lib/python3.12/multiprocessing/util.py:456\u001b[39m, in \u001b[36mspawnv_passfds\u001b[39m\u001b[34m(path, args, passfds)\u001b[39m\n\u001b[32m    454\u001b[39m errpipe_read, errpipe_write = os.pipe()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_posixsubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassfds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrpipe_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_USE_VFORK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    462\u001b[39m     os.close(errpipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()  # ensure training mode each epoch\n",
    "\n",
    "    # ----- TRAINING -----\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass → logits\n",
    "        logits = model(images)\n",
    "\n",
    "        # compute loss\n",
    "        computed_loss = loss(logits, labels)\n",
    "\n",
    "        # backward + step\n",
    "        computed_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        train_loss += computed_loss.item() * images.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # ----- VALIDATION -----\n",
    "    model.eval()  # Enter evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            computed_loss = loss(logits, labels)\n",
    "\n",
    "            val_loss += computed_loss.item() * images.size(0)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    save_dir = f\"checkpoints/e{epochs}_b{batch_size}_lr{learning_rate}\"\n",
    "\n",
    "    # Save best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"{save_dir}/best_model.pt\")\n",
    "        print(\"New best val loss: model saved!\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "        f\"| Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
